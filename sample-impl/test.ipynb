{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lokesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading namedentityio: Package 'namedentityio' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Downloading package stopwords to /home/lokesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lokesh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/lokesh/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/lokesh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "  # Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('namedentityio')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(paragraph):\n",
    "    \"\"\"Extracts potential keywords from a paragraph string.\n",
    "\n",
    "    Args:\n",
    "        paragraph: The paragraph string to extract keywords from.\n",
    "\n",
    "    Returns:\n",
    "        A list of potential keywords (frequent words and named entities).\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the paragraph (split into words)\n",
    "    tokens = nltk.word_tokenize(paragraph.lower())\n",
    "\n",
    "    # Filter out stopwords (common words like \"the\", \"a\", \"is\")\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    print(stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Identify named entities (locations, organizations, people)\n",
    "    named_entities = nltk.ne_chunk(nltk.pos_tag(filtered_tokens))\n",
    "\n",
    "    # Extract potential keywords: frequent words and named entities\n",
    "    keywords = Counter(filtered_tokens).most_common(10)  # Top 10 most frequent words\n",
    "    for entity in named_entities:\n",
    "        if (\n",
    "            isinstance(entity[0], str) and entity[1] != \"O\"\n",
    "        ):  # Check for named entity types except 'O' (outside)\n",
    "            keywords.append((entity[0], 1))  # Add named entity with frequency 1\n",
    "\n",
    "    # Return list of keyword tuples (word, frequency)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      ". 3\n",
      "( 2\n",
      "ai 2\n",
      ") 2\n",
      ", 2\n",
      "computers 2\n",
      "language 2\n",
      "artificial 1\n",
      "intelligence 1\n",
      "rapidly 1\n",
      "artificial 1\n",
      "intelligence 1\n",
      "( 1\n",
      "ai 1\n",
      ") 1\n",
      "rapidly 1\n",
      "transforming 1\n",
      "world 1\n",
      ". 1\n",
      "machine 1\n",
      "learning 1\n",
      ", 1\n",
      "subfield 1\n",
      "ai 1\n",
      ", 1\n",
      "enabling 1\n",
      "computers 1\n",
      "learn 1\n",
      "without 1\n",
      "explicit 1\n",
      "programming 1\n",
      ". 1\n",
      "natural 1\n",
      "language 1\n",
      "processing 1\n",
      "( 1\n",
      "nlp 1\n",
      ") 1\n",
      "allows 1\n",
      "computers 1\n",
      "understand 1\n",
      "generate 1\n",
      "human 1\n",
      "language 1\n",
      ". 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "paragraph = \"Artificial intelligence (AI) is rapidly transforming the world. Machine learning, a subfield of AI, is enabling computers to learn without explicit programming. Natural language processing (NLP) allows computers to understand and generate human language.\"\n",
    "keywords = extract_keywords(paragraph)\n",
    "\n",
    "for keyword, frequency in keywords:\n",
    "    print(keyword, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
